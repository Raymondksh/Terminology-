{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b23029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Libraries Installation\n",
    "!pip install PyMuPDF nltk scikit-learn pandas rdflib stopwords flask-ngrok spacy inflect\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df3abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract words from target pdf and contrast pdf\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "def remove_url_func(text):\n",
    "    return re.sub(r'(?:(ftp|http|https)?:\\/\\/)?(?:[\\w-]+\\.)+([a-z]|[A-Z]|[0-9]){2,6}', '', text)\n",
    "\n",
    "def remove_email_func(text):\n",
    "    return re.sub(r'[\\w\\.\\-\\_]+@[\\w\\.\\-\\_]+', '', text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def truncate_words(text, max_words):\n",
    "    return ' '.join(text.split()[:max_words])\n",
    "\n",
    "def substitute_dash(text):\n",
    "    return re.sub(r'-', ' ', text)\n",
    "\n",
    "#def substitute_ampersand(text):\n",
    "    #return text.replace(\"&\", \"and\")\n",
    "\n",
    "def extract_text_from_pdfs(pdf_directory, maximum):\n",
    "    total_words = 0\n",
    "    all_texts = []\n",
    "    if not os.path.exists(pdf_directory):\n",
    "        raise FileNotFoundError(f\"The directory {pdf_directory} does not exist.\")\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            if total_words >= maximum:\n",
    "                break\n",
    "                \n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "            \n",
    "            text = remove_url_func(text)\n",
    "            text = remove_email_func(text)\n",
    "            text = remove_digits(text)\n",
    "            #text = substitute_ampersand(text)\n",
    "            text = substitute_dash(text)\n",
    "            \n",
    "            word_count = count_words(text)\n",
    "            if total_words + word_count > maximum:\n",
    "                word_count = maximum - total_words\n",
    "                text = truncate_words(text, word_count)\n",
    "                assert word_count == count_words(text)\n",
    "            \n",
    "            total_words += word_count\n",
    "            all_texts.append(text)\n",
    "            doc.close()\n",
    "    return all_texts, total_words\n",
    "\n",
    "\n",
    "pdf_directory_target = '/Users/kyawsoehan/Desktop/pdf'\n",
    "pdf_directory_contrast = '/Users/kyawsoehan/Desktop/contrastingpdf'\n",
    "\n",
    "# Extract target texts\n",
    "texts_target, target_words = extract_text_from_pdfs(pdf_directory_target, 1E9)\n",
    "print(f\"Total words in target corpus: {target_words}\")\n",
    "\n",
    "# Extract contrast texts with truncation\n",
    "texts_contrast, contrast_words = extract_text_from_pdfs(pdf_directory_contrast, target_words)\n",
    "print(f\"Total words in contrast corpus: {contrast_words}\")\n",
    "\n",
    "for i, text in enumerate(texts_target):\n",
    "    print(f\"Text from Target PDF {i+1}:\\n{text}\\n\")\n",
    "for i, text in enumerate(texts_contrast):\n",
    "    print(f\"Text from Contrast PDF {i+1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5944f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data Cleansing with Stopword \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    stop_words = set(stopwords.words('english')) - {\"and\"}\n",
    "    stop_words = stop_words.union({\",\", \";\", \"ii\", \"iii\", \".\", \"(\", \")\", \"!\", \"@\", \"£\", \"$\", \"%\", \"^\", \"*\", \"-\", \"+\", \"/\"})\n",
    "    #stop_words = {\",\", \";\", \"ii\", \".\", \"(\", \")\", \"!\", \"@\", \"£\", \"$\", \"%\", \"^\", \"*\", \"-\", \"+\", \"/\", \"‘\", \"’\"}\n",
    "    filtered_texts = []\n",
    "    for text in texts:\n",
    "        words = word_tokenize(text)\n",
    "        filtered_text = ' '.join([word for word in words if not word.lower() in stop_words])\n",
    "        filtered_texts.append(filtered_text)\n",
    "    return filtered_texts\n",
    "\n",
    "clean_texts_target = remove_stopwords(texts_target)\n",
    "clean_texts_contrast = remove_stopwords(texts_contrast)\n",
    "\n",
    "for i, clean_text in enumerate(clean_texts_target):\n",
    "    print(f\"Cleaned Target Text {i+1}:\\n{clean_text}\\n\")\n",
    "for i, clean_text in enumerate(clean_texts_contrast):\n",
    "    print(f\"Cleaned Contrast Text {i+1}:\\n{clean_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8214580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate and count N-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    # Tokenize the cleaned text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Generate n-grams\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    \n",
    "    # Count the frequency of each n-gram\n",
    "    n_gram_freq = Counter(n_grams)\n",
    "    \n",
    "    return n_gram_freq\n",
    "\n",
    "# Extract and print bigrams and trigrams for the cleaned texts\n",
    "for i, clean_text in enumerate(clean_texts_target):\n",
    "    bigrams = generate_ngrams(clean_text, 2)\n",
    "    trigrams = generate_ngrams(clean_text, 3)\n",
    "    print(f\"Target Text {i+1} Bigrams:\\n{bigrams}\\n\")\n",
    "    print(f\"Target Text {i+1} Trigrams:\\n{trigrams}\\n\")\n",
    "\n",
    "for i, clean_text in enumerate(clean_texts_contrast):\n",
    "    bigrams = generate_ngrams(clean_text, 2)\n",
    "    trigrams = generate_ngrams(clean_text, 3)\n",
    "    print(f\"Contrast Text {i+1} Bigrams:\\n{bigrams}\\n\")\n",
    "    print(f\"Contrast Text {i+1} Trigrams:\\n{trigrams}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb75992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Noun Phrases using SpaCy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_noun_phrases_spacy(texts):\n",
    "    noun_phrases = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        noun_phrases.extend([chunk.text for chunk in doc.noun_chunks])\n",
    "    return noun_phrases\n",
    "\n",
    "noun_phrases_target = extract_noun_phrases_spacy(clean_texts_target)\n",
    "noun_phrases_contrast = extract_noun_phrases_spacy(clean_texts_contrast)\n",
    "\n",
    "print(f\"Noun Phrases (Target):\\n{noun_phrases_target}\\n\")\n",
    "print(f\"Noun Phrases (Contrast):\\n{noun_phrases_contrast}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate TF-IDF and Extract Top Terms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate TF-IDF\n",
    "def calculate_tfidf(phrases, ngram_range=(1, 4)):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=ngram_range)\n",
    "    tfidf_matrix = vectorizer.fit_transform(phrases)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# For Ngrams\n",
    "tfidf_matrix_ngram_target, feature_names_ngram_target = calculate_tfidf(clean_texts_target, ngram_range=(1, 4))\n",
    "tfidf_matrix_ngram_contrast, feature_names_ngram_contrast = calculate_tfidf(clean_texts_contrast, ngram_range=(1, 4))\n",
    "\n",
    "# For Noun Phrases\n",
    "tfidf_matrix_noun_phrase_target, feature_names_noun_phrase_target = calculate_tfidf(noun_phrases_target, ngram_range=(1, 4))\n",
    "tfidf_matrix_noun_phrase_contrast, feature_names_noun_phrase_contrast = calculate_tfidf(noun_phrases_contrast, ngram_range=(1, 4))\n",
    "\n",
    "# Function to extract top terms\n",
    "def extract_top_terms(tfidf_matrix, feature_names, top_n=10):\n",
    "    top_terms = {}\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        df = pd.DataFrame(tfidf_matrix[i].T.todense(), index=feature_names, columns=[\"TF-IDF\"])\n",
    "        df = df.sort_values('TF-IDF', ascending=False)\n",
    "        top_terms[i] = df.head(top_n)\n",
    "    return top_terms\n",
    "\n",
    "# Extract top terms for target and contrast for ngrams and noun phrases\n",
    "top_terms_ngram_target = extract_top_terms(tfidf_matrix_ngram_target, feature_names_ngram_target, top_n=100)\n",
    "top_terms_ngram_contrast = extract_top_terms(tfidf_matrix_ngram_contrast, feature_names_ngram_contrast, top_n=100)\n",
    "top_terms_noun_target = extract_top_terms(tfidf_matrix_noun_phrase_target, feature_names_noun_phrase_target, top_n=100)\n",
    "top_terms_noun_contrast = extract_top_terms(tfidf_matrix_noun_phrase_contrast, feature_names_noun_phrase_contrast, top_n=100)\n",
    "\n",
    "# Print top terms for ngrams (Target)\n",
    "print(\"Top terms from Target (Ngrams):\")\n",
    "for doc_id, terms in top_terms_ngram_target.items():\n",
    "    print(f\"Document {doc_id+1}:\")\n",
    "    print(terms)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print top terms for ngrams (Contrast)\n",
    "print(\"Top terms from Contrast (Ngrams):\")\n",
    "for doc_id, terms in top_terms_ngram_contrast.items():\n",
    "    print(f\"Document {doc_id+1}:\")\n",
    "    print(terms)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print top terms for noun phrases (Target)\n",
    "print(\"Top terms from Target (Noun Phrases):\")\n",
    "for doc_id, terms in top_terms_noun_target.items():\n",
    "    print(f\"Document {doc_id+1}:\")\n",
    "    print(terms)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print top terms for noun phrases (Contrast)\n",
    "print(\"Top terms from Contrast (Noun Phrases):\")\n",
    "for doc_id, terms in top_terms_noun_contrast.items():\n",
    "    print(f\"Document {doc_id+1}:\")\n",
    "    print(terms)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Domain Relevence, Combine score of TF-IDF and DR and Aggregate Top 100 terms for ngrams and nouns \n",
    "from collections import Counter\n",
    "import inflect\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from functools import reduce\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Initialize inflect engine for singular/plural conversion\n",
    "p = inflect.engine()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to aggregate top terms from TF-IDF matrix\n",
    "def aggregate_top_terms(tfidf_matrix, feature_names):\n",
    "    all_terms = Counter()\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        df = pd.DataFrame(tfidf_matrix[i].T.todense(), index=feature_names, columns=[\"TF-IDF\"])\n",
    "        df = df.sort_values('TF-IDF', ascending=False)\n",
    "        for term, score in df.itertuples():\n",
    "            all_terms[term] += score\n",
    "    return all_terms\n",
    "\n",
    "# Function to calculate domain relevance\n",
    "def calculate_term_frequencies(phrases):\n",
    "    all_words = ' '.join(phrases).split()\n",
    "    return reduce(lambda a,b:a+b,[FreqDist(ngrams(all_words, n)) for n in range(1,4)])\n",
    "\n",
    "def calculate_domain_relevance(domain_freqs, contrasting_freqs):\n",
    "    dr_scores = {}\n",
    "    for term, freq in domain_freqs.items():\n",
    "        dr_scores[term] = freq / (contrasting_freqs[term] + 1)  # Add 1 to avoid division by zero\n",
    "    return dr_scores\n",
    "\n",
    "def combine_scores(tfidf_scores, dr_scores):\n",
    "    combined_scores = {}\n",
    "    for term, score in tfidf_scores.items():\n",
    "        combined_scores[term] = score * dr_scores.get(term, 1)  # Use DR score if available, otherwise use 1\n",
    "    return combined_scores\n",
    "\n",
    "def filter_and_sort_terms(aggregated_terms, top_n=100):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_terms = {term: score for term, score in aggregated_terms.items() if term.lower() not in stop_words and not term.isdigit()and len(term) > 1}\n",
    "    \n",
    "    # top and tail stop words\n",
    "    f = {}\n",
    "    for term, score in aggregated_terms.items():\n",
    "        t = term.split()\n",
    "        while t!=[] and t[0] in stop_words:\n",
    "            t = t[1:]\n",
    "        while t!=[] and t[-1] in stop_words:\n",
    "            t.pop()\n",
    "        if t!=[]:\n",
    "            f[\" \".join(t)] = score\n",
    "    filtered_terms = f\n",
    "    sorted_terms = Counter(filtered_terms).most_common()\n",
    "    \n",
    "    filtered_list = []\n",
    "    printed_terms = set()\n",
    "    count = 0\n",
    "    \n",
    "    for term, score in sorted_terms:\n",
    "        if count >= top_n:\n",
    "            break\n",
    "        word = term.split()[0]\n",
    "        singular_term = p.singular_noun(word) or word\n",
    "        stemmed_term = stemmer.stem(singular_term)\n",
    "        lemmatized_term = lemmatizer.lemmatize(singular_term)\n",
    "        \n",
    "        if stemmed_term not in printed_terms and lemmatized_term not in printed_terms:\n",
    "            filtered_list.append((term, score))\n",
    "            printed_terms.add(stemmed_term)\n",
    "            printed_terms.add(lemmatized_term)\n",
    "            count += 1\n",
    "    return filtered_list\n",
    "\n",
    "# Aggregate top terms for ngrams and noun phrases\n",
    "aggregated_terms_ngram = aggregate_top_terms(tfidf_matrix_ngram_target, feature_names_ngram_target)\n",
    "aggregated_terms_noun = aggregate_top_terms(tfidf_matrix_noun_phrase_target, feature_names_noun_phrase_target)\n",
    "\n",
    "# Calculate domain relevance using contrast documents\n",
    "domain_freqs_ngram = calculate_term_frequencies(clean_texts_target)\n",
    "contrasting_freqs_ngram = calculate_term_frequencies(clean_texts_contrast)\n",
    "\n",
    "dr_scores_ngram = calculate_domain_relevance(domain_freqs_ngram, contrasting_freqs_ngram)\n",
    "\n",
    "domain_freqs_noun = calculate_term_frequencies(noun_phrases_target)\n",
    "contrasting_freqs_noun = calculate_term_frequencies(noun_phrases_contrast)\n",
    "\n",
    "dr_scores_noun = calculate_domain_relevance(domain_freqs_noun, contrasting_freqs_noun)\n",
    "\n",
    "# Combine scores for ngrams and noun phrases\n",
    "combined_scores_ngram = combine_scores(aggregated_terms_ngram, dr_scores_ngram)\n",
    "combined_scores_noun = combine_scores(aggregated_terms_noun, dr_scores_noun)\n",
    "\n",
    "# Filter and sort top terms for ngrams and noun phrases\n",
    "filtered_final_scores_ngram = filter_and_sort_terms(combined_scores_ngram, top_n=100)\n",
    "filtered_final_scores_noun = filter_and_sort_terms(combined_scores_noun, top_n=100)\n",
    "\n",
    "# Print the top terms with scores for ngrams and noun phrases\n",
    "def print_terms(label, filtered_terms):\n",
    "    print(f\"Filtered and Aggregated Top Terms ({label}):\")\n",
    "    for term, score in filtered_terms:\n",
    "        print(f\"  {term}: {score:.4f}\")\n",
    "\n",
    "print_terms(\"Ngrams - Target\", filtered_final_scores_ngram)\n",
    "print_terms(\"Noun Phrases - Target\", filtered_final_scores_noun)\n",
    "\n",
    "# Save the combined scores for further use\n",
    "with open('filtered_final_scores_ngram_target.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_final_scores_ngram, f)\n",
    "\n",
    "with open('filtered_final_scores_noun_target.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_final_scores_noun, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065de9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SKOS Exporting \n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import SKOS\n",
    "from urllib.parse import quote\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from functools import reduce\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "def create_skos_combined(ngram_terms, noun_terms, sentences, base_uri):\n",
    "    g = Graph()\n",
    "    skos = Namespace(\"http://www.w3.org/2004/02/skos/core#\")\n",
    "    \n",
    "    # Define the concept schemes for ngrams and nouns\n",
    "    scheme_ngrams = URIRef(f\"{base_uri}/ngrams/concept_scheme\")\n",
    "    scheme_nouns = URIRef(f\"{base_uri}/nouns/concept_scheme\")\n",
    "    g.add((scheme_ngrams, RDF.type, skos.ConceptScheme))\n",
    "    g.add((scheme_ngrams, SKOS.prefLabel, Literal(\"ngrams\")))\n",
    "    g.add((scheme_nouns, RDF.type, skos.ConceptScheme))\n",
    "    g.add((scheme_nouns, SKOS.prefLabel, Literal(\"nouns\")))\n",
    "\n",
    "    seen_terms = {}\n",
    "\n",
    "    # Add ngrams terms and their details\n",
    "    for term, score in ngram_terms:\n",
    "        if term not in seen_terms:\n",
    "            seen_terms[term] = {'ngram_score': score, 'noun_score': 0}\n",
    "        else:\n",
    "            seen_terms[term]['ngram_score'] = score\n",
    "\n",
    "    # Add noun phrases terms and their details\n",
    "    for term, score in noun_terms:\n",
    "        if term not in seen_terms:\n",
    "            seen_terms[term] = {'ngram_score': 0, 'noun_score': score}\n",
    "        else:\n",
    "            seen_terms[term]['noun_score'] = score\n",
    "\n",
    "    # Process terms and add to graph\n",
    "    for term, scores in seen_terms.items():\n",
    "        s = [s for s in sentences if term in s.lower()]\n",
    "        s.sort(key=lambda s: len(s))\n",
    "        sentence = s[len(s)//2] if s else \"No example sentence available.\"\n",
    "        \n",
    "        encoded_term = quote(term)\n",
    "        concept = URIRef(f\"{base_uri}/concept/{encoded_term}\")\n",
    "        g.add((concept, RDF.type, skos.Concept))\n",
    "        g.add((concept, SKOS.prefLabel, Literal(term, lang='en-GB')))\n",
    "        scope_note = f\"Ngram Score: {scores['ngram_score']:.4f}, Noun Score: {scores['noun_score']:.4f}\"\n",
    "        g.add((concept, SKOS.scopeNote, Literal(scope_note, lang='en-GB')))\n",
    "        g.add((concept, SKOS.definition, Literal(sentence, lang='en-GB')))\n",
    "        if scores['ngram_score'] > 0:\n",
    "            g.add((concept, SKOS.inScheme, scheme_ngrams))\n",
    "        if scores['noun_score'] > 0:\n",
    "            g.add((concept, SKOS.inScheme, scheme_nouns))\n",
    "\n",
    "    return g\n",
    "\n",
    "# Base URI for the SKOS vocabulary\n",
    "base_uri = \"http://localhost/tematres/vocab\"\n",
    "\n",
    "# Assuming `sentences` and `filtered_terms_ngram`, `filtered_terms_noun` are already defined and available\n",
    "sentences = reduce(lambda x, y: x + y, [sent_tokenize(t, language='english') for t in texts_target])\n",
    "sentences = [re.sub(r'\\n', ' ', s) for s in sentences]\n",
    "\n",
    "# Load filtered terms for n-gram from the pickle file\n",
    "with open('filtered_final_scores_ngram_target.pkl', 'rb') as f:\n",
    "    filtered_terms_ngram = pickle.load(f)\n",
    "\n",
    "# Load filtered terms for noun phrases from the pickle file\n",
    "with open('filtered_final_scores_noun_target.pkl', 'rb') as f:\n",
    "    filtered_terms_noun = pickle.load(f)\n",
    "\n",
    "# Create combined SKOS RDF for n-grams and noun phrases\n",
    "skos_graph_combined = create_skos_combined(filtered_terms_ngram, filtered_terms_noun, sentences, base_uri)\n",
    "\n",
    "# Serialize SKOS RDF to XML format and print\n",
    "skos_xml_combined = skos_graph_combined.serialize(format='xml', encoding='utf-8').decode('utf-8')\n",
    "print(\"SKOS RDF Output for Combined N-grams and Noun Phrases:\\n\")\n",
    "print(skos_xml_combined)\n",
    "\n",
    "# Save the combined SKOS RDF to a file\n",
    "output_file_path = '/Users/kyawsoehan/Desktop/skos_update_files/skos_output_combined.rdf'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(skos_xml_combined)\n",
    "\n",
    "print(f\"SKOS RDF file for Combined N-grams and Noun Phrases saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ad763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagged Text Output for Ngrams and Nouns for localhost to import TemaTres System \n",
    "import re\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from functools import reduce\n",
    "\n",
    "def create_tagged_text(ngram_terms, noun_terms, sentences, concept_scheme_ngram, concept_scheme_noun):\n",
    "    output = [f\"{concept_scheme_ngram}\", f\"{concept_scheme_noun}\"]\n",
    "    seen_terms = set()\n",
    "\n",
    "    # Add ngrams terms and their details\n",
    "    for term, score in ngram_terms:\n",
    "        if term not in seen_terms:\n",
    "            # Word-level matching for sentences\n",
    "            matching_sentences = [s for s in sentences if all(word in word_tokenize(s) for word in term.split())]\n",
    "            matching_sentences.sort(key=lambda s: len(s))\n",
    "            sentence = matching_sentences[len(matching_sentences)//2] if matching_sentences else \"No example sentence available.\"\n",
    "\n",
    "            output.append(f\"\\n{term}\")\n",
    "            output.append(f\"\\tTT: {concept_scheme_ngram}\")\n",
    "            output.append(f\"\\tNA: Ngram Score: {score:.4f}\")\n",
    "            output.append(f\"\\tDF: {sentence}\")\n",
    "            output.append(f\"\\tBT: {concept_scheme_ngram}\")\n",
    "            seen_terms.add(term)\n",
    "        else:\n",
    "            output.append(f\"\\n{term}\")\n",
    "            output.append(f\"\\tTT: {concept_scheme_ngram}\")\n",
    "            output.append(f\"\\tNA: Ngram Score: {score:.4f}\")\n",
    "            output.append(f\"\\tBT: {concept_scheme_ngram}\")\n",
    "\n",
    "    # Add noun phrases terms and their details\n",
    "    for term, score in noun_terms:\n",
    "        if term not in seen_terms:\n",
    "            # Word-level matching for sentences\n",
    "            matching_sentences = [s for s in sentences if all(word in word_tokenize(s) for word in term.split())]\n",
    "            matching_sentences.sort(key=lambda s: len(s))\n",
    "            sentence = matching_sentences[len(matching_sentences)//2] if matching_sentences else \"No example sentence available.\"\n",
    "\n",
    "            output.append(f\"\\n{term}\")\n",
    "            output.append(f\"\\tTT: {concept_scheme_noun}\")\n",
    "            output.append(f\"\\tNA: Noun Score: {score:.4f}\")\n",
    "            output.append(f\"\\tDF: {sentence}\")\n",
    "            output.append(f\"\\tBT: {concept_scheme_noun}\")\n",
    "            seen_terms.add(term)\n",
    "        else:\n",
    "            output.append(f\"\\n{term}\")\n",
    "            output.append(f\"\\tTT: {concept_scheme_noun}\")\n",
    "            output.append(f\"\\tNA: Noun Score: {score:.4f}\")\n",
    "            output.append(f\"\\tBT: {concept_scheme_noun}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# Example usage\n",
    "concept_scheme_ngram = \"ngrams\"\n",
    "concept_scheme_noun = \"nouns\"\n",
    "base_uri = \"http://localhost/tematres/vocab\"\n",
    "\n",
    "# Assuming `sentences`, `filtered_terms_ngram`, and `filtered_terms_noun` are already defined and available\n",
    "sentences = reduce(lambda x, y: x + y, [sent_tokenize(t, language='english') for t in texts_target])\n",
    "sentences = [re.sub(r'\\n', ' ', s) for s in sentences]\n",
    "\n",
    "\n",
    "# Load filtered terms for ngrams and nouns from the pickle files\n",
    "with open('filtered_final_scores_ngram_target.pkl', 'rb') as f:\n",
    "    filtered_terms_ngram = pickle.load(f)\n",
    "\n",
    "with open('filtered_final_scores_noun_target.pkl', 'rb') as f:\n",
    "    filtered_terms_noun = pickle.load(f)\n",
    "\n",
    "# Create tagged text output for both ngrams and nouns\n",
    "tagged_text_output = create_tagged_text(filtered_terms_ngram, filtered_terms_noun, sentences, concept_scheme_ngram, concept_scheme_noun)\n",
    "\n",
    "# Print the tagged text output\n",
    "print(tagged_text_output)\n",
    "\n",
    "# Save the tagged text output to a file\n",
    "output_file_path = '/Users/kyawsoehan/Desktop/skos_update_files/tagged_text_output_combined_local.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(tagged_text_output)\n",
    "\n",
    "print(f\"Tagged text file for Ngrams and Nouns saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagged Text Output for Ngrams and Nouns for herokuhost\n",
    "import re\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from functools import reduce\n",
    "\n",
    "def create_tagged_text(ngram_terms, noun_terms, sentences, concept_scheme_ngram, concept_scheme_noun):\n",
    "    output = [f\"{concept_scheme_ngram}\", f\"{concept_scheme_noun}\"]\n",
    "    seen_terms = set()\n",
    "\n",
    "    # Add ngrams terms and their details\n",
    "    for term, score in ngram_terms:\n",
    "        if term not in seen_terms:\n",
    "            # Word-level matching for sentences\n",
    "            matching_sentences = [s for s in sentences if all(word in word_tokenize(s) for word in term.split())]\n",
    "            matching_sentences.sort(key=lambda s: len(s))\n",
    "            sentence = matching_sentences[len(matching_sentences)//2] if matching_sentences else \"No example sentence available.\"\n",
    "\n",
    "            output.append(f\"\\n{term}\")\n",
    "            output.append(f\"\\tTT: {concept_scheme_ngram}\")\n",
    "            output.append(f\"\\tNA: Ngram Score: {score:.4f}\")\n",
    "            output.append(f\"\\tDF: {sentence}\")\n",
    "            output.append(f\"\\tBT: {concept_scheme_ngram}\")\n",
    "            seen_terms.add(term)\n",
    "        else:\n",
    "            output.append(f\"\\n{term}\")\n",
    "            output.append(f\"\\tTT: {concept_scheme_ngram}\")\n",
    "            output.append(f\"\\tNA: Ngram Score: {score:.4f}\")\n",
    "            output.append(f\"\\tBT: {concept_scheme_ngram}\")\n",
    "\n",
    "    # Add noun phrases terms and their details\n",
    "    for term, score in noun_terms:\n",
    "        if term not in seen_terms:\n",
    "            # Word-level matching for sentences\n",
    "            matching_sentences = [s for s in sentences if all(word in word_tokenize(s) for word in term.split())]\n",
    "            matching_sentences.sort(key=lambda s: len(s))\n",
    "            sentence = matching_sentences[len(matching_sentences)//2] if matching_sentences else \"No example sentence available.\"\n",
    "\n",
    "            output.append(f\"\\n{term}\")\n",
    "            output.append(f\"\\tTT: {concept_scheme_noun}\")\n",
    "            output.append(f\"\\tNA: Noun Score: {score:.4f}\")\n",
    "            output.append(f\"\\tDF: {sentence}\")\n",
    "            output.append(f\"\\tBT: {concept_scheme_noun}\")\n",
    "            seen_terms.add(term)\n",
    "        else:\n",
    "            output.append(f\"\\n{term}\")\n",
    "            output.append(f\"\\tTT: {concept_scheme_noun}\")\n",
    "            output.append(f\"\\tNA: Noun Score: {score:.4f}\")\n",
    "            output.append(f\"\\tBT: {concept_scheme_noun}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# Example usage\n",
    "concept_scheme_ngram = \"ngrams\"\n",
    "concept_scheme_noun = \"nouns\"\n",
    "base_uri = \"http://corporate-terminology-uwe-163e1432bf51.herokuapp.com/vocab/\"\n",
    "\n",
    "# Assuming `sentences`, `filtered_terms_ngram`, and `filtered_terms_noun` are already defined and available\n",
    "sentences = reduce(lambda x, y: x + y, [sent_tokenize(t, language='english') for t in texts_target])\n",
    "sentences = [re.sub(r'\\n', ' ', s) for s in sentences]\n",
    "\n",
    "# Load filtered terms for ngrams and nouns from the pickle files\n",
    "with open('filtered_final_scores_ngram_target.pkl', 'rb') as f:\n",
    "    filtered_terms_ngram = pickle.load(f)\n",
    "\n",
    "with open('filtered_final_scores_noun_target.pkl', 'rb') as f:\n",
    "    filtered_terms_noun = pickle.load(f)\n",
    "\n",
    "# Create tagged text output for both ngrams and nouns\n",
    "tagged_text_output = create_tagged_text(filtered_terms_ngram, filtered_terms_noun, sentences, concept_scheme_ngram, concept_scheme_noun)\n",
    "\n",
    "# Print the tagged text output\n",
    "print(tagged_text_output)\n",
    "\n",
    "# Save the tagged text output to a file\n",
    "output_file_path = '/Users/kyawsoehan/Desktop/skos_update_files/tagged_text_output_combined_heroku.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(tagged_text_output)\n",
    "\n",
    "print(f\"Tagged text file for Ngrams and Nouns saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b2f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
